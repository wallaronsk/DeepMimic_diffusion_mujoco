{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_loaders.dataset import MujocoMotionDataset\n",
    "dataset = MujocoMotionDataset(\"../data/motions\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([121, 35]), torch.Size([121, 34]), 'roll')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_motion = dataset[0]\n",
    "pos, vel, label = first_motion\n",
    "pos.shape, vel.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # not used in the final model\n",
    "        x = x + self.pe[:x.shape[0], :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(self, latent_dim, sequence_pos_encoder):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_pos_encoder = sequence_pos_encoder\n",
    "\n",
    "        time_embed_dim = self.latent_dim\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        return self.time_embed(self.sequence_pos_encoder.pe[timesteps]).permute(1, 0, 2)\n",
    "    \n",
    "class MotionTransformer(nn.Module):\n",
    "    def __init__(self, njoints, nfeats, latent_dim=256, ff_size=1024, num_layers=8, num_heads=4, dropout=0.1, activation=\"gelu\"):\n",
    "        super(MotionTransformer, self).__init__()\n",
    "        \n",
    "        self.njoints = njoints\n",
    "        self.nfeats = nfeats\n",
    "        self.input_dim = njoints * nfeats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.ff_size = ff_size  \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.poseEmbedding = nn.Linear(self.nfeats, self.latent_dim)\n",
    "        self.velEmbedding = nn.Linear(self.nfeats, self.latent_dim)\n",
    "        self.sequence_pos_encoder = PositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.embed_timestep = TimestepEmbedder(self.latent_dim, self.sequence_pos_encoder)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=self.latent_dim, nhead=num_heads, \n",
    "                                                    dim_feedforward=ff_size, dropout=dropout, activation=activation)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        # Output Linear Layer\n",
    "        self.poseFinal = nn.Linear(self.latent_dim, self.nfeats)\n",
    "        self.velFinal = nn.Linear(self.latent_dim, self.nfeats)\n",
    "\n",
    "    def forward(self, x, timesteps, y=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, njoints, nfeats, max_frames], denoted x_t in the paper\n",
    "        timesteps: [batch_size] (int)\n",
    "        \"\"\"\n",
    "        # x: [batch_size, njoints, nfeats, seq_len]\n",
    "        bs, n_joints, n_feats, n_frames = x.shape\n",
    "        emb = self.embed_timestep(timesteps)  # [1, bs, d]\n",
    "\n",
    "        # Input process\n",
    "        x = x.permute(3, 0, 1, 2).reshape(n_frames, bs, self.input_dim)  # [n_frames, batch_size, input_dim]\n",
    "        first_pose = x[[0]]  # [1, bs, 150]\n",
    "        first_pose = self.poseEmbedding(first_pose)  # [1, bs, d]\n",
    "        vel = x[1:]  # [n_frames-1, bs, 150]\n",
    "        vel = self.velEmbedding(vel)  # [n_frames-1, bs, d]\n",
    "        x = torch.cat((first_pose, vel), axis=0)  # [n_frames, bs, d]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # adding the timestep embed\n",
    "        xseq = torch.cat((emb, x), axis=0)  # [n_frames+1, bs, d]\n",
    "        xseq = self.sequence_pos_encoder(xseq)  # [n_frames+1, bs, d]\n",
    "        output = self.transformer_encoder(xseq)[1:]  # , src_key_padding_mask=~maskseq)  # [n_frames, bs, d]\n",
    "\n",
    "        # Output Linear\n",
    "        first_pose = output[[0]]  # [1, bs, d]\n",
    "        first_pose = self.poseFinal(first_pose)  # [1, bs, 150]\n",
    "        vel = output[1:]  # [n_frames-1, bs, d]\n",
    "        vel = self.velFinal(vel)  # [n_frames-1, bs, 150]\n",
    "        output = torch.cat((first_pose, vel), axis=0)  # [n_frames, bs, 150]\n",
    "\n",
    "        # Reshape to original format\n",
    "        x = x.permute(1, 2, 0).reshape(bs, self.njoints, self.nfeats, n_frames)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
