{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup env\n",
    "This is notebook specific setup, my module path is different on my vm so this is a workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "You can change the type of motion by changing the filepath\n",
    "Dataset right now just repeats the same motion 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmp angle [0.0, 0.0, 0.85536, 0.9966429999999997, -0.0070009999999999795, 0.08157, 0.0005729999999999971, 0.042303731260289315, -0.056088768155961526, -0.01172717680484046, -0.014103614145860938, 0.2358842735659614, 0.37124889801787253, -0.6111023347690597, -0.09268300376873025, -0.09541896434572254, 0.585361, 0.1699928747321186, 0.08652758875118252, 0.354108626550405, 0.160215, -0.2285399691330798, -0.39445967594673703, -0.1178224382194308, -0.369571, 0.20448116583595066, -0.12115992907931128, 0.07892319943485762, 0.3736623102073797, -0.010008232584494297, 0.30603690929303384, -0.364281, -0.13425257761871864, -0.004787718949892447, 0.0010873114649849894] 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " Batch(trajectories=tensor([[ 0.0000,  0.0000,  0.8554,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0037, -0.0062,  0.8563,  ..., -0.2435, -1.1484, -0.8920],\n",
       "         [ 0.0064, -0.0122,  0.8575,  ..., -0.0278, -1.2998, -0.9550],\n",
       "         ...,\n",
       "         [-0.2466,  2.0540,  0.8465,  ..., -0.8048,  0.5575,  1.2816],\n",
       "         [-0.2435,  2.0658,  0.8467,  ..., -0.7320,  0.5210,  1.2596],\n",
       "         [-0.2394,  2.0782,  0.8469,  ..., -0.5900,  0.5328,  1.0961]]), conditions={0: tensor([ 0.0000e+00,  0.0000e+00,  8.5536e-01,  9.9664e-01, -7.0010e-03,\n",
       "          8.1570e-02,  5.7300e-04,  4.2304e-02, -5.6089e-02, -1.1727e-02,\n",
       "         -1.4104e-02,  2.3588e-01,  3.7125e-01, -6.1110e-01, -9.2683e-02,\n",
       "         -9.5419e-02,  5.8536e-01,  1.6999e-01,  8.6528e-02,  3.5411e-01,\n",
       "          1.6022e-01, -2.2854e-01, -3.9446e-01, -1.1782e-01, -3.6957e-01,\n",
       "          2.0448e-01, -1.2116e-01,  7.8923e-02,  3.7366e-01, -1.0008e-02,\n",
       "          3.0604e-01, -3.6428e-01, -1.3425e-01, -4.7877e-03,  1.0873e-03,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])}),\n",
       " torch.Size([160, 69]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusion.data_loaders.motion_dataset_v2 import MotionDataset\n",
    "dataset = MotionDataset(\"data/motions/humanoid3d_cartwheel.txt\")\n",
    "len(dataset), dataset[0], dataset[0].trajectories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model\n",
    "Configure your experiment name and savepaths here, they will all be stored under the logs folder later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusion.diffuser.utils import Trainer as dTrainer, Config as dConfig\n",
    "\n",
    "exp_name = \"test-cartwheel-shuffled-128\"\n",
    "savepath = f'/home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/{exp_name}'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "    os.makedirs(os.path.join(savepath, 'sampled_motions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[utils/config ] Config: <class 'diffusion.diffuser.models.temporal_v2.TemporalUnet'>\n",
      "    cond_dim: 69\n",
      "    horizon: 160\n",
      "    transition_dim: 69\n",
      "\n",
      "[ utils/config ] Saved config to: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel-shuffled-128/model_config.pkl\n",
      "\n",
      "[ models/temporal ] Channel dimensions: [(69, 128), (128, 256), (256, 512), (512, 1024)]\n",
      "[(69, 128), (128, 256), (256, 512), (512, 1024)]\n"
     ]
    }
   ],
   "source": [
    "from diffusion.diffuser.models.temporal_v2 import TemporalUnet \n",
    "\n",
    "horizon = dataset[0].trajectories.shape[0]\n",
    "transition_dim = dataset[0].trajectories.shape[1]\n",
    "\n",
    "model_config = dConfig(\n",
    "    TemporalUnet,\n",
    "    savepath=(savepath, 'model_config.pkl'),\n",
    "    horizon=horizon,\n",
    "    transition_dim=transition_dim,\n",
    "    cond_dim=transition_dim,\n",
    "    device=device,\n",
    ")\n",
    "model = model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[utils/config ] Config: <class 'diffusion.diffuser.models.diffusion_v2.GaussianDiffusion'>\n",
      "    action_dim: 34\n",
      "    action_weight: 5\n",
      "    clip_denoised: False\n",
      "    horizon: 160\n",
      "    loss_discount: 1\n",
      "    loss_type: l2\n",
      "    loss_weights: None\n",
      "    n_timesteps: 1000\n",
      "    observation_dim: 35\n",
      "    predict_epsilon: False\n",
      "\n",
      "[ utils/config ] Saved config to: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel-shuffled-128/diffusion_config.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from diffusion.diffuser.models.diffusion_v2 import GaussianDiffusion\n",
    "\n",
    "# model params, I am only using the very basic ones, some params are for conditioning\n",
    "n_timesteps = 1000\n",
    "loss_type = 'l2'\n",
    "clip_denoised = False\n",
    "predict_epsilon = False\n",
    "action_weight = 5\n",
    "loss_weights = None\n",
    "loss_discount = 1\n",
    "pos_dim = 35\n",
    "vel_dim = 34\n",
    "\n",
    "diffusion_config = dConfig(\n",
    "    GaussianDiffusion,\n",
    "    savepath=(savepath, \"diffusion_config.pkl\"),\n",
    "    horizon=horizon,\n",
    "    # transition_dim=transition_dim,\n",
    "    observation_dim=pos_dim,\n",
    "    action_dim=vel_dim,\n",
    "    n_timesteps=n_timesteps,\n",
    "    loss_type=loss_type,\n",
    "    clip_denoised=clip_denoised,\n",
    "    predict_epsilon=predict_epsilon,\n",
    "    # loss weighting\n",
    "    action_weight=action_weight,\n",
    "    loss_weights=loss_weights,\n",
    "    loss_discount=loss_discount,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "diffusion = diffusion_config(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[utils/config ] Config: <class 'diffusion.diffuser.utils.training.Trainer'>\n",
      "    bucket: None\n",
      "    ema_decay: 0.995\n",
      "    gradient_accumulate_every: 2\n",
      "    label_freq: 20000\n",
      "    n_reference: 8\n",
      "    results_folder: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel-shuffled-128\n",
      "    sample_freq: 2000\n",
      "    save_freq: 2000\n",
      "    save_parallel: False\n",
      "    train_batch_size: 32\n",
      "    train_lr: 0.0002\n",
      "\n",
      "[ utils/config ] Saved config to: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel-shuffled-128/trainer_config.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-4\n",
    "gradient_accumulate_every = 2\n",
    "ema_decay = 0.995\n",
    "sample_freq = 2000\n",
    "save_freq = 2000\n",
    "n_train_steps = 1e5\n",
    "n_saves = 5\n",
    "save_parallel = False\n",
    "bucket = None\n",
    "n_reference = 8\n",
    "train_batch_size = 32\n",
    "\n",
    "trainer_config = dConfig(\n",
    "    dTrainer,\n",
    "    savepath=(savepath, 'trainer_config.pkl'),\n",
    "    train_batch_size=train_batch_size,\n",
    "    train_lr=learning_rate,\n",
    "    gradient_accumulate_every=gradient_accumulate_every,\n",
    "    ema_decay=ema_decay,\n",
    "    sample_freq=sample_freq,\n",
    "    save_freq=save_freq,\n",
    "    label_freq=int(n_train_steps // n_saves),\n",
    "    save_parallel=save_parallel,\n",
    "    results_folder=savepath,\n",
    "    bucket=bucket,\n",
    "    n_reference=n_reference,\n",
    ")\n",
    "\n",
    "trainer = trainer_config(diffusion, dataset, renderer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model and training loop works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([1, 160, 35])\n",
      "x.shape torch.Size([1, 35, 160])\n",
      "x.shape torch.Size([1, 128, 160])\n",
      "1\n",
      "xfinal.shape torch.Size([1, 128, 80])\n",
      "x.shape torch.Size([1, 256, 80])\n",
      "2\n",
      "xfinal.shape torch.Size([1, 256, 40])\n",
      "x.shape torch.Size([1, 512, 40])\n",
      "3\n",
      "xfinal.shape torch.Size([1, 512, 20])\n",
      "x.shape torch.Size([1, 1024, 20])\n",
      "4\n",
      "xfinal.shape torch.Size([1, 1024, 20])\n",
      "xt1.shape torch.Size([1, 1024, 20])\n",
      "xt2.shape torch.Size([1, 1024, 20])\n",
      "xt3.shape torch.Size([1, 1024, 20])\n",
      "pop.shape torch.Size([1, 1024, 20])\n",
      "pop.shape torch.Size([1, 512, 40])\n",
      "pop.shape torch.Size([1, 256, 80])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0948, -0.0691,  0.0749,  ..., -0.1594, -0.1720,  0.0441],\n",
       "          [-0.2168,  0.0640,  0.1174,  ..., -0.0011, -0.0825,  0.0775],\n",
       "          [-0.2648, -0.0748,  0.1949,  ..., -0.3040, -0.1790,  0.1174],\n",
       "          ...,\n",
       "          [-0.1973,  0.0216,  0.3098,  ..., -0.1037,  0.0096,  0.2913],\n",
       "          [-0.2072,  0.1525, -0.1111,  ..., -0.0610,  0.3798,  0.3302],\n",
       "          [-0.2301,  0.0057,  0.0998,  ..., -0.0250, -0.0661,  0.0190]]],\n",
       "        device='cuda:0', grad_fn=<PermuteBackward0>),\n",
       " torch.Size([1, 160, 35]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffuser.utils import batchify\n",
    "\n",
    "tunet = model\n",
    "tunet.to(device)\n",
    "test_data = dataset[0]\n",
    "batch = batchify(test_data)\n",
    "t = torch.randint(0, 1000, (1,), device=device).long().to(device)\n",
    "res = tunet(batch.trajectories, cond=batch.conditions, time=t, verbose=True)\n",
    "res, res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ utils/arrays ] Total parameters: 3.96 M\n",
      "         downs.3.0.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.3.1.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.3.1.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         ups.0.0.blocks.0.block.0.weight: 327.68 k | Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block1.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block1.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block2.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block2.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.3.0.blocks.0.block.0.weight: 163.84 k | Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.2.0.blocks.1.block.0.weight: 81.92 k | Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         ... and 186 others accounting for 1.09 M parameters\n",
      "Testing forward... ✓\n"
     ]
    }
   ],
   "source": [
    "from diffuser.utils import report_parameters, batchify\n",
    "\n",
    "report_parameters(model)\n",
    "\n",
    "print('Testing forward...', end=' ', flush=True)\n",
    "x = dataset[0]\n",
    "batch = batchify(x)\n",
    "loss, _ = diffusion.loss(batch.trajectories, {})\n",
    "loss.backward()\n",
    "print('✓')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "It took me 80s to run 1 epoch and results were pretty good from just 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Epoch 0 / 1 | /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel\n",
      "[ utils/training ] Saved model to /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel/state_0.pt\n",
      "0:   0.3086 | a0_loss:   0.0269 | t:   0.4321\n",
      "100:   0.0675 | a0_loss:   0.0111 | t:   8.1653\n",
      "200:   0.0279 | a0_loss:   0.0061 | t:   8.1041\n",
      "300:   0.0137 | a0_loss:   0.0037 | t:   7.9897\n",
      "400:   0.0073 | a0_loss:   0.0024 | t:   8.1874\n",
      "500:   0.0044 | a0_loss:   0.0016 | t:   8.3597\n",
      "600:   0.0030 | a0_loss:   0.0011 | t:   8.4126\n",
      "700:   0.0022 | a0_loss:   0.0008 | t:   8.2396\n",
      "800:   0.0017 | a0_loss:   0.0006 | t:   8.4983\n",
      "900:   0.0014 | a0_loss:   0.0004 | t:   8.5166\n",
      "[ utils/training ] Saved model to /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel/state_1.pt\n"
     ]
    }
   ],
   "source": [
    "n_steps_per_epoch = 1000\n",
    "n_epochs = int(n_train_steps // n_steps_per_epoch)\n",
    "print(n_epochs)\n",
    "n_epochs = 1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(f'Epoch {i} / {n_epochs} | {savepath}')\n",
    "    trainer.train(n_train_steps=n_steps_per_epoch)\n",
    "\n",
    "trainer.save(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Alternatively) load a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusion.diffuser.utils import load_diffusion\n",
    "# diffusion_experiment = load_diffusion(\n",
    "#     \"/home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/diffuser\", dataset=dataset, epoch=1)\n",
    "\n",
    "# renderer = diffusion_experiment.renderer\n",
    "# model = diffusion_experiment.trainer.ema_model\n",
    "\n",
    "# Optionally load a checkpoint\n",
    "trainer.load(3)\n",
    "model = trainer.ema_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from model\n",
    "We want to see what the model does if it starts with a motion, could it convert one motion to another? Or blend them in someway?\n",
    "\n",
    "Start from existing motion data instead of noise, we want to mix walking and doing a cartwheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmp angle [0.0, 0.0, 0.847532, 0.998678, 0.01410399999999995, 0.049422999999999995, -0.0006980000000000042, 0.019374995056800275, 0.008037254877450587, -0.09523902811084285, -0.0, 0.0, -0.0, -0.15553532463598624, 0.23919429256424163, 0.20739656997070288, 0.170571, 0.3529631848273465, -0.2610682953696931, -0.24560532144975333, 0.581348, 0.02035205257945668, -0.5175742452141794, -0.11376339039728192, -0.249116, 0.020556236630260034, -0.019534498786735962, 0.0655269812790598, -0.05606350142619236, 0.15209578259548684, 0.1827420948157945, -0.391532, 0.1931167851688944, -0.2978918547932108, -0.08305715225197069] 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " Batch(trajectories=tensor([[ 0.0000,  0.0000,  0.8475,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0415, -0.0047,  0.8466,  ...,  0.4419, -0.9102,  1.1832],\n",
       "         [ 0.0817, -0.0105,  0.8483,  ...,  1.9768, -2.7716,  1.5138],\n",
       "         ...,\n",
       "         [ 0.9535,  0.0164,  0.8741,  ..., -0.0242,  0.5358,  0.5449],\n",
       "         [ 0.9810,  0.0152,  0.8736,  ...,  0.0819,  0.5424,  0.2741],\n",
       "         [ 1.0080,  0.0138,  0.8721,  ..., -0.0949,  0.5196,  0.1573]]), conditions={0: tensor([ 0.0000e+00,  0.0000e+00,  8.4753e-01,  9.9868e-01,  1.4104e-02,\n",
       "          4.9423e-02, -6.9800e-04,  1.9375e-02,  8.0373e-03, -9.5239e-02,\n",
       "         -0.0000e+00,  0.0000e+00, -0.0000e+00, -1.5554e-01,  2.3919e-01,\n",
       "          2.0740e-01,  1.7057e-01,  3.5296e-01, -2.6107e-01, -2.4561e-01,\n",
       "          5.8135e-01,  2.0352e-02, -5.1757e-01, -1.1376e-01, -2.4912e-01,\n",
       "          2.0556e-02, -1.9534e-02,  6.5527e-02, -5.6064e-02,  1.5210e-01,\n",
       "          1.8274e-01, -3.9153e-01,  1.9312e-01, -2.9789e-01, -8.3057e-02,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])}),\n",
       " torch.Size([32, 69]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusion.data_loaders.motion_dataset_v2 import MotionDataset\n",
    "\n",
    "walk_dataset = MotionDataset(\"data/motions/humanoid3d_walk.txt\")\n",
    "len(walk_dataset), walk_dataset[0], walk_dataset[0].trajectories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the walk to be the same shape as the cartwheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 69])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_traj = walk_dataset[0].trajectories\n",
    "walk_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion saved as test2.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 160, 69])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_traj = walk_dataset[0].trajectories\n",
    "walk_traj = torch.cat([walk_traj] * 5, dim=0)\n",
    "pos_diff = walk_traj[-1, :3] - walk_traj[0, :3]\n",
    "walk_traj[32:, :3] += pos_diff[:]\n",
    "walk_traj[64:, :3] += pos_diff[:]\n",
    "walk_traj[96:, :3] += pos_diff[:]\n",
    "walk_traj[128:, :3] += pos_diff[:]\n",
    "import numpy as np\n",
    "\n",
    "savepath = \"/home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/0-test\"\n",
    "\n",
    "\n",
    "def save_motions(sample, output_dir, filename=\"motion.npy\"):\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    pos_data = walk_traj\n",
    "    pos_data = pos_data[:, :35].cpu().numpy()\n",
    "    np.save(filepath, pos_data)\n",
    "    print(f\"Motion saved as {filename}\")\n",
    "\n",
    "\n",
    "save_motions(\n",
    "    None,\n",
    "    f\"{savepath}\",\n",
    "    filename=\"test2.npy\",\n",
    ")\n",
    "walk_traj.unsqueeze_(0)\n",
    "walk_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[F                                                                                                    \n",
      "\u001b[F1 / 2 [##############################                              ]  50% | 13.1 Hz\n",
      "t : 1 | vmax : 0.0 | vmin : 0.0\n",
      "\u001b[F\u001b[F                                                                                                    \n",
      "                                                                                                    \n",
      "\u001b[F\u001b[F2 / 2 [############################################################] 100% | 22.0 Hz\n",
      "t : 0 | vmax : 0.0 | vmin : 0.0\n",
      "\u001b[F\u001b[F                                                                                                    \n",
      "                                                                                                    \n",
      "\u001b[F\u001b[F[ Progress ] 2 / 2 | t : 0 | vmax : 0.0 | vmin : 0.0 | 22.0 Hz\n"
     ]
    }
   ],
   "source": [
    "from diffuser.utils import batchify\n",
    "test = dataset[0]\n",
    "batch = batchify(test)\n",
    "sample = diffusion.p_sample_loop(batch.trajectories.shape, batch.conditions, starting_motion=walk_traj, max_timesteps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion saved as starting-walk-motion.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def save_motions(sample, output_dir, filename=\"motion.npy\"):\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    pos_data = sample.trajectories.squeeze(0)[:, :35].cpu().numpy()\n",
    "    np.save(filepath, pos_data)\n",
    "    print(f\"Motion saved as {filename}\")\n",
    "\n",
    "\n",
    "save_motions(sample, f\"{savepath}/sampled_motions\", filename=\"starting-walk-motion.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
