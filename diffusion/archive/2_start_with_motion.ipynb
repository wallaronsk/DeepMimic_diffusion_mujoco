{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup env\n",
    "This is notebook specific setup, my module path is different on my vm so this is a workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "You can change the type of motion by changing the filepath\n",
    "Dataset right now just repeats the same motion 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmp angle [0.0, 0.0, 0.85536, 0.9966429999999997, -0.0070009999999999795, 0.08157, 0.0005729999999999971, 0.042303731260289315, -0.056088768155961526, -0.01172717680484046, -0.014103614145860938, 0.2358842735659614, 0.37124889801787253, -0.6111023347690597, -0.09268300376873025, -0.09541896434572254, 0.585361, 0.1699928747321186, 0.08652758875118252, 0.354108626550405, 0.160215, -0.2285399691330798, -0.39445967594673703, -0.1178224382194308, -0.369571, 0.20448116583595066, -0.12115992907931128, 0.07892319943485762, 0.3736623102073797, -0.010008232584494297, 0.30603690929303384, -0.364281, -0.13425257761871864, -0.004787718949892447, 0.0010873114649849894] 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " Batch(trajectories=tensor([[ 0.0000e+00,  0.0000e+00,  8.5536e-01,  ..., -1.3425e-01,\n",
       "          -4.7877e-03,  1.0873e-03],\n",
       "         [ 3.6500e-03, -6.2240e-03,  8.5632e-01,  ..., -1.3036e-01,\n",
       "           1.4385e-02,  1.5935e-02],\n",
       "         [ 6.4000e-03, -1.2205e-02,  8.5748e-01,  ..., -1.3041e-01,\n",
       "           3.6052e-02,  3.1853e-02],\n",
       "         ...,\n",
       "         [-2.4664e-01,  2.0540e+00,  8.4651e-01,  ...,  5.9186e-02,\n",
       "          -2.0091e-01,  2.3318e-01],\n",
       "         [-2.4351e-01,  2.0658e+00,  8.4672e-01,  ...,  7.3301e-02,\n",
       "          -2.0666e-01,  2.1504e-01],\n",
       "         [-2.3938e-01,  2.0782e+00,  8.4690e-01,  ...,  8.5008e-02,\n",
       "          -2.1333e-01,  1.9921e-01]]), conditions={0: tensor([ 0.0000e+00,  0.0000e+00,  8.5536e-01,  9.9664e-01, -7.0010e-03,\n",
       "          8.1570e-02,  5.7300e-04,  4.2304e-02, -5.6089e-02, -1.1727e-02,\n",
       "         -1.4104e-02,  2.3588e-01,  3.7125e-01, -6.1110e-01, -9.2683e-02,\n",
       "         -9.5419e-02,  5.8536e-01,  1.6999e-01,  8.6528e-02,  3.5411e-01,\n",
       "          1.6022e-01, -2.2854e-01, -3.9446e-01, -1.1782e-01, -3.6957e-01,\n",
       "          2.0448e-01, -1.2116e-01,  7.8923e-02,  3.7366e-01, -1.0008e-02,\n",
       "          3.0604e-01, -3.6428e-01, -1.3425e-01, -4.7877e-03,  1.0873e-03])}),\n",
       " torch.Size([160, 35]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusion.data_loaders.motion_dataset import MotionDataset\n",
    "dataset = MotionDataset(\"data/motions/humanoid3d_cartwheel.txt\")\n",
    "len(dataset), dataset[0], dataset[0].trajectories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model\n",
    "Configure your experiment name and savepaths here, they will all be stored under the logs folder later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusion.diffuser.utils import Trainer as dTrainer, Config as dConfig\n",
    "\n",
    "exp_name = \"test-cartwheel\"\n",
    "savepath = f'/home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/{exp_name}'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "    os.makedirs(os.path.join(savepath, 'sampled_motions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenji/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[utils/config ] Config: <class 'diffusion.diffuser.models.temporal.TemporalUnet'>\n",
      "    cond_dim: 35\n",
      "    horizon: 160\n",
      "    transition_dim: 35\n",
      "\n",
      "[ utils/config ] Saved config to: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel/model_config.pkl\n",
      "\n",
      "[ models/temporal ] Channel dimensions: [(35, 32), (32, 64), (64, 128), (128, 256)]\n",
      "[(35, 32), (32, 64), (64, 128), (128, 256)]\n"
     ]
    }
   ],
   "source": [
    "from diffusion.diffuser.models.temporal import TemporalUnet \n",
    "\n",
    "horizon = dataset[0].trajectories.shape[0]\n",
    "transition_dim = dataset[0].trajectories.shape[1]\n",
    "\n",
    "model_config = dConfig(\n",
    "    TemporalUnet,\n",
    "    savepath=(savepath, 'model_config.pkl'),\n",
    "    horizon=horizon,\n",
    "    transition_dim=transition_dim,\n",
    "    cond_dim=transition_dim,\n",
    "    device=device,\n",
    ")\n",
    "model = model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[utils/config ] Config: <class 'diffusion.diffuser.models.diffusion.GaussianDiffusion'>\n",
      "    action_weight: 5\n",
      "    clip_denoised: False\n",
      "    horizon: 160\n",
      "    loss_discount: 1\n",
      "    loss_type: l2\n",
      "    loss_weights: None\n",
      "    n_timesteps: 100\n",
      "    predict_epsilon: False\n",
      "    transition_dim: 35\n",
      "\n",
      "[ utils/config ] Saved config to: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel/diffusion_config.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from diffusion.diffuser.models.diffusion import GaussianDiffusion\n",
    "\n",
    "# model params, I am only using the very basic ones, some params are for conditioning\n",
    "n_timesteps = 100\n",
    "loss_type = 'l2'\n",
    "clip_denoised = False\n",
    "predict_epsilon = False\n",
    "action_weight = 5\n",
    "loss_weights = None\n",
    "loss_discount = 1\n",
    "\n",
    "diffusion_config = dConfig(\n",
    "    GaussianDiffusion,\n",
    "    savepath=(savepath, 'diffusion_config.pkl'),\n",
    "    horizon=horizon,\n",
    "    transition_dim=transition_dim,\n",
    "    n_timesteps=n_timesteps,\n",
    "    loss_type=loss_type,\n",
    "    clip_denoised=clip_denoised,\n",
    "    predict_epsilon=predict_epsilon,\n",
    "    # loss weighting\n",
    "    action_weight=action_weight,\n",
    "    loss_weights=loss_weights,\n",
    "    loss_discount=loss_discount,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "diffusion = diffusion_config(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[utils/config ] Config: <class 'diffusion.diffuser.utils.training.Trainer'>\n",
      "    bucket: None\n",
      "    ema_decay: 0.995\n",
      "    gradient_accumulate_every: 2\n",
      "    label_freq: 20000\n",
      "    n_reference: 8\n",
      "    results_folder: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel\n",
      "    sample_freq: 2000\n",
      "    save_freq: 2000\n",
      "    save_parallel: False\n",
      "    train_batch_size: 32\n",
      "    train_lr: 0.0002\n",
      "\n",
      "[ utils/config ] Saved config to: /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel/trainer_config.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-4\n",
    "gradient_accumulate_every = 2\n",
    "ema_decay = 0.995\n",
    "sample_freq = 2000\n",
    "save_freq = 2000\n",
    "n_train_steps = 1e5\n",
    "n_saves = 5\n",
    "save_parallel = False\n",
    "bucket = None\n",
    "n_reference = 8\n",
    "train_batch_size = 32\n",
    "\n",
    "trainer_config = dConfig(\n",
    "    dTrainer,\n",
    "    savepath=(savepath, 'trainer_config.pkl'),\n",
    "    train_batch_size=train_batch_size,\n",
    "    train_lr=learning_rate,\n",
    "    gradient_accumulate_every=gradient_accumulate_every,\n",
    "    ema_decay=ema_decay,\n",
    "    sample_freq=sample_freq,\n",
    "    save_freq=save_freq,\n",
    "    label_freq=int(n_train_steps // n_saves),\n",
    "    save_parallel=save_parallel,\n",
    "    results_folder=savepath,\n",
    "    bucket=bucket,\n",
    "    n_reference=n_reference,\n",
    ")\n",
    "\n",
    "trainer = trainer_config(diffusion, dataset, renderer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model and training loop works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([1, 160, 35])\n",
      "x.shape torch.Size([1, 35, 160])\n",
      "x.shape torch.Size([1, 32, 160])\n",
      "1\n",
      "xfinal.shape torch.Size([1, 32, 80])\n",
      "x.shape torch.Size([1, 64, 80])\n",
      "2\n",
      "xfinal.shape torch.Size([1, 64, 40])\n",
      "x.shape torch.Size([1, 128, 40])\n",
      "3\n",
      "xfinal.shape torch.Size([1, 128, 20])\n",
      "x.shape torch.Size([1, 256, 20])\n",
      "4\n",
      "xfinal.shape torch.Size([1, 256, 20])\n",
      "xt1.shape torch.Size([1, 256, 20])\n",
      "xt2.shape torch.Size([1, 256, 20])\n",
      "xt3.shape torch.Size([1, 256, 20])\n",
      "pop.shape torch.Size([1, 256, 20])\n",
      "pop.shape torch.Size([1, 128, 40])\n",
      "pop.shape torch.Size([1, 64, 80])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0944,  0.1104, -0.1531,  ...,  0.1012,  0.2474,  0.4012],\n",
       "          [-0.0605,  0.5519, -0.4045,  ..., -0.2518,  0.2368, -0.3138],\n",
       "          [ 0.0369, -0.1111,  0.0324,  ...,  0.1454,  0.3190,  0.3811],\n",
       "          ...,\n",
       "          [-0.0291,  0.3816, -0.2992,  ...,  0.2897,  0.4946,  0.1762],\n",
       "          [ 0.2478, -0.2832,  0.0067,  ...,  0.2888,  0.1798,  0.4154],\n",
       "          [ 0.3851,  0.4280,  0.2402,  ...,  0.1765,  0.0856,  0.0039]]],\n",
       "        device='cuda:0', grad_fn=<PermuteBackward0>),\n",
       " torch.Size([1, 160, 35]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffuser.utils import batchify\n",
    "\n",
    "tunet = model\n",
    "tunet.to(device)\n",
    "test_data = dataset[0]\n",
    "batch = batchify(test_data)\n",
    "t = torch.randint(0, 1000, (1,), device=device).long().to(device)\n",
    "res = tunet(batch.trajectories, cond=batch.conditions, time=t, verbose=True)\n",
    "res, res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ utils/arrays ] Total parameters: 3.96 M\n",
      "         downs.3.0.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.3.1.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.3.1.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         ups.0.0.blocks.0.block.0.weight: 327.68 k | Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block1.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block1.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block2.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         mid_block2.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.3.0.blocks.0.block.0.weight: 163.84 k | Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         downs.2.0.blocks.1.block.0.weight: 81.92 k | Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "         ... and 186 others accounting for 1.09 M parameters\n",
      "Testing forward... ✓\n"
     ]
    }
   ],
   "source": [
    "from diffuser.utils import report_parameters, batchify\n",
    "\n",
    "report_parameters(model)\n",
    "\n",
    "print('Testing forward...', end=' ', flush=True)\n",
    "x = dataset[0]\n",
    "batch = batchify(x)\n",
    "loss, _ = diffusion.loss(batch.trajectories, {})\n",
    "loss.backward()\n",
    "print('✓')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "It took me 80s to run 1 epoch and results were pretty good from just 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Epoch 0 / 1 | /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel\n",
      "[ utils/training ] Saved model to /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel/state_0.pt\n",
      "0:   0.3086 | a0_loss:   0.0269 | t:   0.4321\n",
      "100:   0.0675 | a0_loss:   0.0111 | t:   8.1653\n",
      "200:   0.0279 | a0_loss:   0.0061 | t:   8.1041\n",
      "300:   0.0137 | a0_loss:   0.0037 | t:   7.9897\n",
      "400:   0.0073 | a0_loss:   0.0024 | t:   8.1874\n",
      "500:   0.0044 | a0_loss:   0.0016 | t:   8.3597\n",
      "600:   0.0030 | a0_loss:   0.0011 | t:   8.4126\n",
      "700:   0.0022 | a0_loss:   0.0008 | t:   8.2396\n",
      "800:   0.0017 | a0_loss:   0.0006 | t:   8.4983\n",
      "900:   0.0014 | a0_loss:   0.0004 | t:   8.5166\n",
      "[ utils/training ] Saved model to /home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/test-cartwheel/state_1.pt\n"
     ]
    }
   ],
   "source": [
    "n_steps_per_epoch = 1000\n",
    "n_epochs = int(n_train_steps // n_steps_per_epoch)\n",
    "print(n_epochs)\n",
    "n_epochs = 1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(f'Epoch {i} / {n_epochs} | {savepath}')\n",
    "    trainer.train(n_train_steps=n_steps_per_epoch)\n",
    "\n",
    "trainer.save(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Alternatively) load a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusion.diffuser.utils import load_diffusion\n",
    "# diffusion_experiment = load_diffusion(\n",
    "#     \"/home/kenji/Fyp/DeepMimic_mujoco/diffusion/logs/diffuser\", dataset=dataset, epoch=1)\n",
    "\n",
    "# renderer = diffusion_experiment.renderer\n",
    "# model = diffusion_experiment.trainer.ema_model\n",
    "\n",
    "# Optionally load a checkpoint\n",
    "trainer.load(1)\n",
    "model = trainer.ema_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from model\n",
    "We want to see what the model does if it starts with a motion, could it convert one motion to another? Or blend them in someway?\n",
    "\n",
    "Start from existing motion data instead of noise, we want to mix walking and doing a cartwheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmp angle [0.0, 0.0, 0.847532, 0.998678, 0.01410399999999995, 0.049422999999999995, -0.0006980000000000042, 0.019374995056800275, 0.008037254877450587, -0.09523902811084285, -0.0, 0.0, -0.0, -0.15553532463598624, 0.23919429256424163, 0.20739656997070288, 0.170571, 0.3529631848273465, -0.2610682953696931, -0.24560532144975333, 0.581348, 0.02035205257945668, -0.5175742452141794, -0.11376339039728192, -0.249116, 0.020556236630260034, -0.019534498786735962, 0.0655269812790598, -0.05606350142619236, 0.15209578259548684, 0.1827420948157945, -0.391532, 0.1931167851688944, -0.2978918547932108, -0.08305715225197069] 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " Batch(trajectories=tensor([[ 0.0000,  0.0000,  0.8475,  ...,  0.1931, -0.2979, -0.0831],\n",
       "         [ 0.0415, -0.0047,  0.8466,  ...,  0.1812, -0.2662, -0.1258],\n",
       "         [ 0.0817, -0.0105,  0.8483,  ...,  0.1293, -0.1646, -0.1874],\n",
       "         ...,\n",
       "         [ 0.9535,  0.0164,  0.8741,  ...,  0.0799, -0.2361, -0.1045],\n",
       "         [ 0.9810,  0.0152,  0.8736,  ...,  0.0750, -0.2538, -0.1148],\n",
       "         [ 1.0080,  0.0138,  0.8721,  ...,  0.0762, -0.2713, -0.1198]]), conditions={0: tensor([ 0.0000e+00,  0.0000e+00,  8.4753e-01,  9.9868e-01,  1.4104e-02,\n",
       "          4.9423e-02, -6.9800e-04,  1.9375e-02,  8.0373e-03, -9.5239e-02,\n",
       "         -0.0000e+00,  0.0000e+00, -0.0000e+00, -1.5554e-01,  2.3919e-01,\n",
       "          2.0740e-01,  1.7057e-01,  3.5296e-01, -2.6107e-01, -2.4561e-01,\n",
       "          5.8135e-01,  2.0352e-02, -5.1757e-01, -1.1376e-01, -2.4912e-01,\n",
       "          2.0556e-02, -1.9534e-02,  6.5527e-02, -5.6064e-02,  1.5210e-01,\n",
       "          1.8274e-01, -3.9153e-01,  1.9312e-01, -2.9789e-01, -8.3057e-02])}),\n",
       " torch.Size([32, 35]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusion.data_loaders.motion_dataset import MotionDataset\n",
    "\n",
    "walk_dataset = MotionDataset(\"data/motions/humanoid3d_walk.txt\")\n",
    "len(walk_dataset), walk_dataset[0], walk_dataset[0].trajectories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the walk to be the same shape as the cartwheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 160, 35])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_traj = walk_dataset[0].trajectories\n",
    "# padding = torch.zeros((100, 59))\n",
    "walk_traj = torch.cat([walk_traj] * 5, dim=0)\n",
    "walk_traj.unsqueeze_(0)\n",
    "walk_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[F                                                                                                    \n",
      "\u001b[F1 / 1 [############################################################] 100% | 26.8 Hz\n",
      "t : 0 | vmax : 0.0 | vmin : 0.0\n",
      "\u001b[F\u001b[F                                                                                                    \n",
      "                                                                                                    \n",
      "\u001b[F\u001b[F[ Progress ] 1 / 1 | t : 0 | vmax : 0.0 | vmin : 0.0 | 26.8 Hz\n"
     ]
    }
   ],
   "source": [
    "from diffuser.utils import batchify\n",
    "test = dataset[0]\n",
    "batch = batchify(test)\n",
    "sample = diffusion.p_sample_loop(batch.trajectories.shape, batch.conditions, starting_motion=walk_traj, max_timesteps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion saved as mix-walk-motion2.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def save_motions(sample, output_dir, filename=\"motion.npy\"):\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    pos_data = sample.trajectories.squeeze(0).cpu().numpy()\n",
    "    np.save(filepath, pos_data)\n",
    "    print(f\"Motion saved as {filename}\")\n",
    "\n",
    "\n",
    "save_motions(sample, f\"{savepath}/sampled_motions\", filename=\"mix-walk-motion.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
