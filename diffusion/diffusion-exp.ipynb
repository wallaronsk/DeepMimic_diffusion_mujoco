{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_loaders.backflip_dataset import BackflipMotionDataset\n",
    "dataset = BackflipMotionDataset(\"../data/motions\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29, 35]), torch.Size([29, 34]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos, vel = dataset[0]\n",
    "pos.shape, vel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # not used in the final model\n",
    "        x = x + self.pe[:x.shape[0], :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(self, latent_dim, sequence_pos_encoder):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_pos_encoder = sequence_pos_encoder\n",
    "\n",
    "        time_embed_dim = self.latent_dim\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        return self.time_embed(self.sequence_pos_encoder.pe[timesteps]).permute(1, 0, 2)\n",
    "    \n",
    "class MotionTransformer(nn.Module):\n",
    "    def __init__(self, nfeats, latent_dim=256, ff_size=1024, num_layers=8, num_heads=4, dropout=0.1, activation=\"gelu\"):\n",
    "        super(MotionTransformer, self).__init__()\n",
    "        \n",
    "        self.nfeats = nfeats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.ff_size = ff_size  \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.poseEmbedding = nn.Linear(self.nfeats, self.latent_dim)\n",
    "        self.velEmbedding = nn.Linear(self.nfeats, self.latent_dim)\n",
    "        self.sequence_pos_encoder = PositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.embed_timestep = TimestepEmbedder(self.latent_dim, self.sequence_pos_encoder)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=self.latent_dim, nhead=num_heads, \n",
    "                                                    dim_feedforward=ff_size, dropout=dropout, activation=activation)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        # Output Linear Layer\n",
    "        self.poseFinal = nn.Linear(self.latent_dim, self.nfeats)\n",
    "        self.velFinal = nn.Linear(self.latent_dim, self.nfeats)\n",
    "\n",
    "    def forward(self, x, timesteps, y=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, njoints, nfeats, max_frames], denoted x_t in the paper\n",
    "        timesteps: [batch_size] (int)\n",
    "        \"\"\"\n",
    "        # x: [batch_size, njoints, nfeats, seq_len]\n",
    "        bs, n_joints, n_feats, n_frames = x.shape\n",
    "        emb = self.embed_timestep(timesteps)  # [1, bs, d]\n",
    "\n",
    "        # Input process\n",
    "        x = x.permute(3, 0, 1, 2).reshape(n_frames, bs, self.input_dim)  # [n_frames, batch_size, input_dim]\n",
    "        first_pose = x[[0]]  # [1, bs, 150]\n",
    "        first_pose = self.poseEmbedding(first_pose)  # [1, bs, d]\n",
    "        vel = x[1:]  # [n_frames-1, bs, 150]\n",
    "        vel = self.velEmbedding(vel)  # [n_frames-1, bs, d]\n",
    "        x = torch.cat((first_pose, vel), axis=0)  # [n_frames, bs, d]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # adding the timestep embed\n",
    "        xseq = torch.cat((emb, x), axis=0)  # [n_frames+1, bs, d]\n",
    "        xseq = self.sequence_pos_encoder(xseq)  # [n_frames+1, bs, d]\n",
    "        output = self.transformer_encoder(xseq)[1:]  # , src_key_padding_mask=~maskseq)  # [n_frames, bs, d]\n",
    "\n",
    "        # Output Linear\n",
    "        first_pose = output[[0]]  # [1, bs, d]\n",
    "        first_pose = self.poseFinal(first_pose)  # [1, bs, 150]\n",
    "        vel = output[1:]  # [n_frames-1, bs, d]\n",
    "        vel = self.velFinal(vel)  # [n_frames-1, bs, 150]\n",
    "        output = torch.cat((first_pose, vel), axis=0)  # [n_frames, bs, 150]\n",
    "\n",
    "        # Reshape to original format\n",
    "        x = x.permute(1, 2, 0).reshape(bs, self.njoints, self.nfeats, n_frames)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, drop_last=True)\n",
    "\n",
    "\n",
    "model = MotionTransformer(nfeats=150, latent_dim=32, ff_size=128, num_layers=8, num_heads=4, dropout=0.1, activation=\"gelu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, batch in enumerate(dataloader):\n",
    "    pos, vel = batch\n",
    "    pos.shape, vel.shape\n",
    "    m\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import gaussian_diffusion as gd\n",
    "from diffusion.respace import SpacedDiffusion, space_timesteps\n",
    "\n",
    "def create_gaussian_diffusion(args):\n",
    "    # default params\n",
    "    predict_xstart = True  # we always predict x_start (a.k.a. x0), that's our deal!\n",
    "    steps = args.diffusion_steps\n",
    "    scale_beta = 1.  # no scaling\n",
    "    timestep_respacing = ''  # can be used for ddim sampling, we don't use it.\n",
    "    learn_sigma = False\n",
    "    rescale_timesteps = False\n",
    "\n",
    "    betas = gd.get_named_beta_schedule(args.noise_schedule, steps, scale_beta)\n",
    "    loss_type = gd.LossType.MSE\n",
    "\n",
    "    if not timestep_respacing:\n",
    "        timestep_respacing = [steps]\n",
    "\n",
    "    return SpacedDiffusion( # this class handles the geometric losses, remove foot contact loss\n",
    "        use_timesteps=space_timesteps(steps, timestep_respacing),\n",
    "        betas=betas,\n",
    "        model_mean_type=(\n",
    "            gd.ModelMeanType.EPSILON if not predict_xstart else gd.ModelMeanType.START_X\n",
    "        ),\n",
    "        model_var_type=(\n",
    "            (\n",
    "                gd.ModelVarType.FIXED_LARGE\n",
    "                if not args.sigma_small\n",
    "                else gd.ModelVarType.FIXED_SMALL\n",
    "            )\n",
    "            if not learn_sigma\n",
    "            else gd.ModelVarType.LEARNED_RANGE\n",
    "        ),\n",
    "        loss_type=loss_type,\n",
    "        rescale_timesteps=rescale_timesteps,\n",
    "        lambda_vel=args.lambda_vel,\n",
    "        lambda_rcxyz=args.lambda_rcxyz,\n",
    "        lambda_fc=args.lambda_fc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = train_args()\n",
    "model = MotionTransformer(njoints=14, nfeats=10)\n",
    "diffusion = create_gaussian_diffusion(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
